
# Kolmogorov vs MLP Project

## Overview
This project compares the performance of Kolmogorov Arnold Networks (KAN) and Multi-Layer Perceptrons (MLP) in a specific task. KAN is a type of neural network architecture inspired by the Kolmogorov-Arnold theorem in dynamical systems theory.

## Objectives
- Evaluate the performance of KAN and MLP on a given dataset.
- Compare the training and inference times of both models.
- Analyze the convergence properties and generalization abilities of KAN and MLP.

## Methodology
1. **Data Preparation**: Preprocess the dataset for training and testing.
2. **Model Implementation**: Implement KAN and MLP using PyTorch.
3. **Training**: Train both models using the same dataset.
4. **Evaluation**: Evaluate the models on a separate test dataset.
5. **Comparison**: Compare the performance metrics of KAN and MLP.
6. **Analysis**: Analyze the results to draw conclusions.

## Findings
- KAN showed faster convergence compared to MLP.
- MLP exhibited better generalization on unseen data.
- The training time of KAN was lower than that of MLP.

## Conclusion
In this project, we explored the performance of KAN and MLP in a specific task. While KAN showed faster convergence, MLP demonstrated better generalization. The choice between the two models depends on the specific requirements of the task.

